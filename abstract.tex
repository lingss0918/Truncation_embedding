\begin{abstract}
This paper presents a systematic evaluation of learning or find word embeddings with limited memory. We study the effect of limited precision data representation and computation on word embedding. Our results show that word embedding can use only 8 bit wide fixed-point number representation with no degradeation in the word similarity task. And we also demonstrate methods to directly training these limited precision data representation. 
\end{abstract}

