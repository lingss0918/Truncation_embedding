\section{Related Work}
\label{sec:relatedwork}

{\bf First paragraph}: Brief summary about what you want to review in this selection, e.g., word embedding methods, and model compression in neural networks. Word2vec is an efficient implementation of  the continuous bag-of-words and skip-gram architectures for computing vector representation of words.
\cite{Mikolov132}\cite{MikolovYZ13}

{\bf Second paragraph}: Word embedding.

{\bf Third paragraph}: Model compression. (not sure whether this is the right name, you can search it).
\url{http://arxiv.org/pdf/1502.02551.pdf} deep learning with limited numerical precision introduce stochastic rounding in deep networks training using only 16 bit wide fixed-point representation. Sparse Overcomplete Word Vector Representations  that transform word vectors into sparse (and optionally binary) vectors. "The resulting representations are more similar to the interpretable features typically used in NLP, though they are discovered automatically from raw corpora. Because the vectors are highly sparse, they are computationally easy to work with." \url{http://www.cs.cmu.edu/~mfaruqui/papers/acl15-overcomplete.pdf} 
